{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DyNet Mini-batches RNN Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training RNNs on sequences, there is a need to batch sequences that have different \n",
    "lengths into batches.  This allows the RNN trainer to execute one batch at a time with the same\n",
    "graph, that is constructed for one specific sequence length.\n",
    "\n",
    "The mechanics of batching sequences, padding them, training and then testing is a bit confusing at first.  This notebook illustrates how to perform this mini-batching process in DyNet.\n",
    "\n",
    "The task we illustrate is a sequence classifier: we map sequences to a label.\n",
    "\n",
    "The input consists of pairs $(s_j, label_j)$ where $s_j$ are sequences of words in a vocabulary of size\n",
    "VOCAB_SIZE and $label_j$ are labels in a label set of size NUM_OF_CLASSES.\n",
    "Each sequence of words $s_j$ contains words $w_{ij}, i \\in 1...l_j]$. \n",
    "\n",
    "The architecture consists of the following steps:\n",
    "1. Embed the words in the sequence $s_j$ into vectors of a fixed size: $e_{ij} = E * oneshot(w_{ij})$ \n",
    "of dim EMBEDDING_SIZE.\n",
    "2. Encode the sequence of word embeddings using an LSTM $encoding(s_j) = lstm([e_{ij}, i \\in 1...l_j])$\n",
    "of dim STATE_SIZE\n",
    "3. Pass the resulting LSTM encoding into a softmax dense layer $predict(s_j) = softmax(W \\times encoding(s_j) + b)$ of dim NUM_OF_CLASSES.\n",
    "4. The loss is the cross-entropy loss: $loss_j = cross\\_entropy(predict(s_j), label_j)$\n",
    "\n",
    "<img src='model.jpg' width=50%></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dynet as dn\n",
    "from time import time\n",
    "from numpy import mean, argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 2\n",
    "EMBEDDINGS_SIZE = 10\n",
    "LSTM_NUM_OF_LAYERS = 1\n",
    "STATE_SIZE = 10\n",
    "NUM_OF_CLASSES = 2\n",
    "REPEATS = 1000\n",
    "BATCH_SIZE = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we create a synthetic random dataset, mapping random sequences to labels.\n",
    "The sequences are of variable length from a uniform distribution of lengths [1..max_seq]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 1 seq: [1, 1, 0, 0, 1]\n",
      "label: 0 seq: [0, 0]\n",
      "label: 0 seq: [0, 1, 0, 1, 0]\n",
      "label: 0 seq: [1, 1, 0, 1, 0, 0, 1, 1]\n",
      "label: 0 seq: [1, 1, 1, 0, 1, 1]\n",
      "label: 1 seq: [1, 1, 0, 0]\n",
      "label: 0 seq: [0, 1, 1, 0, 1, 1, 1]\n",
      "label: 0 seq: [0, 1, 1, 0, 0, 1, 1, 1, 1]\n",
      "label: 1 seq: [1, 0, 0, 0, 0]\n",
      "label: 0 seq: [0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "def gen_random_dataset(num_of_examples, max_seq, vocab_size, num_of_classes):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for _ in range(num_of_examples):\n",
    "        seq = []\n",
    "        for _ in range(randint(1, max_seq-1)):\n",
    "            seq.append(randint(0, vocab_size-1))\n",
    "        X.append(seq)\n",
    "        Y.append(randint(0, num_of_classes-1))\n",
    "    return X, Y\n",
    "\n",
    "def print_dataset(X, Y):\n",
    "    for seq, label in zip(X, Y):\n",
    "        print('label:', label, 'seq:', seq)\n",
    "\n",
    "X, Y = gen_random_dataset(10, 10, VOCAB_SIZE, NUM_OF_CLASSES)\n",
    "print_dataset(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Batches "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is the key technical aspect of this notebook.\n",
    "\n",
    "We split our dataset into batches so that we can train each batch with a single RNN of a fixed size.\n",
    "\n",
    "Firstly, we sort the dataset by length so that batches regroup sequences of similar lengths together.  The effect is that the length variance of each batch will be minimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset in batches of size  2\n",
      "label: (0, 1) seq: ([0, 0], [1, 1, 0, 0])\n",
      "label: (1, 0) seq: ([1, 1, 0, 0, 1], [0, 1, 0, 1, 0])\n",
      "label: (1, 0) seq: ([1, 0, 0, 0, 0], [0, 0, 0, 0, 0])\n",
      "label: (0, 0) seq: ([1, 1, 1, 0, 1, 1], [0, 1, 1, 0, 1, 1, 1])\n",
      "label: (0, 0) seq: ([1, 1, 0, 1, 0, 0, 1, 1], [0, 1, 1, 0, 0, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "from numpy import ceil\n",
    "def to_batch(X, Y, batch_size):\n",
    "    #sort dataset by length\n",
    "    data = list(zip(*sorted(zip(X,Y), key=lambda x: len(x[0]))))\n",
    "    batched_X = []\n",
    "    batched_Y = []\n",
    "    for i in range(int(ceil(len(X)/batch_size))):\n",
    "        batched_X.append(data[0][i*batch_size:(i+1)*batch_size])\n",
    "        batched_Y.append(data[1][i*batch_size:(i+1)*batch_size])\n",
    "    return batched_X, batched_Y\n",
    "\n",
    "batched_X, batched_Y = to_batch(X, Y, BATCH_SIZE)\n",
    "print(\"Dataset in batches of size \", BATCH_SIZE)\n",
    "print_dataset(batched_X, batched_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now must ensure that in each batch, all sequences will have the same length.\n",
    "We achieve this by padding the beginning of each sequence with a new vocabulary token which serves as a special code for \"padding\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: (0, 1) seq: [[2, 2, 0, 0], [1, 1, 0, 0]]\n",
      "label: (1, 0) seq: [[1, 1, 0, 0, 1], [0, 1, 0, 1, 0]]\n",
      "label: (1, 0) seq: [[1, 0, 0, 0, 0], [0, 0, 0, 0, 0]]\n",
      "label: (0, 0) seq: [[2, 1, 1, 1, 0, 1, 1], [0, 1, 1, 0, 1, 1, 1]]\n",
      "label: (0, 0) seq: [[2, 1, 1, 0, 1, 0, 0, 1, 1], [0, 1, 1, 0, 0, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "#add new padding token to the vocabulary\n",
    "VOCAB_SIZE += 1\n",
    "\n",
    "def pad_batch(batch):\n",
    "    max_len = len(batch[-1])\n",
    "    padded_batch = []\n",
    "    for x in batch:\n",
    "        x = [VOCAB_SIZE-1]*(max_len-len(x)) + x\n",
    "        padded_batch.append(x)\n",
    "    return padded_batch\n",
    "\n",
    "#pad batches\n",
    "batched_X_padded = list(map(pad_batch, batched_X))\n",
    "print_dataset(batched_X_padded, batched_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DyNet model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is simple: embed each item, run a RNN over all the embedded items, use the last output to classify the sequence with a softmax layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = dn.Model()\n",
    "input_lookup = model.add_lookup_parameters((VOCAB_SIZE, EMBEDDINGS_SIZE))\n",
    "lstm = dn.LSTMBuilder(LSTM_NUM_OF_LAYERS, EMBEDDINGS_SIZE, STATE_SIZE, model)\n",
    "output_w = model.add_parameters((NUM_OF_CLASSES, STATE_SIZE))\n",
    "output_b = model.add_parameters((NUM_OF_CLASSES))\n",
    "\n",
    "# Return the model prediction over a batch - each prediction is a vector (p_i) of dim NUM_OF_CLASSES\n",
    "# As usual we do not apply the softmax here - but in the loss function.\n",
    "def get_probs(batch):\n",
    "    dn.renew_cg()\n",
    "    \n",
    "    # The I iteration embed all the i-th items in all batches\n",
    "    embedded = [dn.lookup_batch(input_lookup, chars) for chars in zip(*batch)]\n",
    "    state = lstm.initial_state()\n",
    "    output_vec = state.transduce(embedded)[-1]\n",
    "    w = dn.parameter(output_w)\n",
    "    b = dn.parameter(output_b)\n",
    "    return w*output_vec+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our function returns scores over a batch, we use 'pickneglogsoftmax_batch' to get the probabilty of all the correct labels over the batch and 'sum_batches' the sum the errors over all these probabilties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(trainX, trainY):\n",
    "    print('starting train')\n",
    "    trainer = dn.AdamTrainer(model)\n",
    "    for _ in range(REPEATS):\n",
    "        for X, Y in zip(trainX, trainY):\n",
    "            probs = get_probs(X)\n",
    "            loss = dn.sum_batches(dn.pickneglogsoftmax_batch(probs, Y))\n",
    "            loss_value = loss.value()\n",
    "            loss.backward()\n",
    "            trainer.update()\n",
    "    print('done training!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to predict over a batch, we use the numpy value of our probabilities (using 'npvalue') and argmax to get the index of the most probable label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate(testX, testY):\n",
    "    print('starting validation')\n",
    "    acc = []\n",
    "    for X, Y in zip(testX, testY):\n",
    "        probs = get_probs(X).npvalue()\n",
    "        for i in range(len(probs[0])):\n",
    "            pred = argmax(probs[:, i])\n",
    "            label = Y[i]\n",
    "            if pred == label:\n",
    "                acc.append(1)\n",
    "            else:\n",
    "                acc.append(0)\n",
    "    print('accuracy: ', mean(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting validation\n",
      "accuracy:  0.7\n",
      "starting train\n",
      "done training!\n",
      "starting validation\n",
      "accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "validate(batched_X_padded, batched_Y)\n",
    "train(batched_X_padded, batched_Y)\n",
    "validate(batched_X_padded, batched_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it: our model learned to classify the sequences.\n",
    "\n",
    "The method of batching + padding is general whenever we use RNNs."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
