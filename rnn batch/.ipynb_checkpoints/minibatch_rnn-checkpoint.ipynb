{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DyNet Mini-batches RNN Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple example of how to use mini-batches for RNNs in DyNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dynet as dn\n",
    "from random import randint\n",
    "from time import time\n",
    "from numpy import mean, argmax, ceil\n",
    "\n",
    "VOCAB_SIZE = 2\n",
    "EMBEDDINGS_SIZE = 10\n",
    "LSTM_NUM_OF_LAYERS = 1\n",
    "STATE_SIZE = 10\n",
    "NUM_OF_CLASSES = 2\n",
    "REPEATS = 1000\n",
    "BATCH_SIZE = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a random sequence to labael dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] 1\n",
      "[0, 1, 1, 1, 1, 1, 0] 0\n",
      "[0, 1, 0, 1, 0, 0, 1, 0, 1] 1\n",
      "[0, 1, 1, 0, 0, 1, 1, 0, 0] 1\n",
      "[0, 1, 0, 0, 0, 0] 0\n",
      "[1] 0\n",
      "[1, 1, 0, 0, 0, 0, 1] 0\n",
      "[1, 1, 1, 0, 0, 1, 0, 0, 0] 0\n",
      "[1, 1, 0, 0, 0, 1, 0, 0] 0\n",
      "[1, 1, 1] 1\n"
     ]
    }
   ],
   "source": [
    "def gen_random_dataset(num_of_examples, max_seq):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for _ in range(num_of_examples):\n",
    "        seq = []\n",
    "        for _ in range(randint(1, max_seq-1)):\n",
    "            seq.append(randint(0, VOCAB_SIZE-1))\n",
    "        X.append(seq)\n",
    "        Y.append(randint(0, NUM_OF_CLASSES-1))\n",
    "    return X, Y\n",
    "\n",
    "def print_dataset(X, Y):\n",
    "    for seq, label in zip(X, Y):\n",
    "        print(seq, label)\n",
    "\n",
    "X, Y = gen_random_dataset(10, 10)\n",
    "print_dataset(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split our dataset to batches, we will sort the dataset by length to make length variance of each batch minimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0], [1]) (1, 0)\n",
      "([1, 1, 1], [0, 1, 0, 0, 0, 0]) (1, 0)\n",
      "([0, 1, 1, 1, 1, 1, 0], [1, 1, 0, 0, 0, 0, 1]) (0, 0)\n",
      "([1, 1, 0, 0, 0, 1, 0, 0], [0, 1, 0, 1, 0, 0, 1, 0, 1]) (0, 1)\n",
      "([0, 1, 1, 0, 0, 1, 1, 0, 0], [1, 1, 1, 0, 0, 1, 0, 0, 0]) (1, 0)\n"
     ]
    }
   ],
   "source": [
    "def to_batch(X, Y):\n",
    "    global VOCAB_SIZE\n",
    "    #sort dataset by length\n",
    "    data = list(zip(*sorted(zip(X,Y), key=lambda x: len(x[0]))))\n",
    "    batched_X = []\n",
    "    batched_Y = []\n",
    "    for i in range(int(ceil(len(X)/BATCH_SIZE))):\n",
    "        batched_X.append(data[0][i*BATCH_SIZE:(i+1)*BATCH_SIZE])\n",
    "        batched_Y.append(data[1][i*BATCH_SIZE:(i+1)*BATCH_SIZE])\n",
    "    return batched_X, batched_Y\n",
    "\n",
    "batched_X, batched_Y = to_batch(X, Y)\n",
    "print_dataset(batched_X, batched_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each batch all sequences must be the same length so we will pad the begining of each sequence with a new vocabilary token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0], [1]] (1, 0)\n",
      "[[2, 2, 2, 1, 1, 1], [0, 1, 0, 0, 0, 0]] (1, 0)\n",
      "[[0, 1, 1, 1, 1, 1, 0], [1, 1, 0, 0, 0, 0, 1]] (0, 0)\n",
      "[[2, 1, 1, 0, 0, 0, 1, 0, 0], [0, 1, 0, 1, 0, 0, 1, 0, 1]] (0, 1)\n",
      "[[0, 1, 1, 0, 0, 1, 1, 0, 0], [1, 1, 1, 0, 0, 1, 0, 0, 0]] (1, 0)\n"
     ]
    }
   ],
   "source": [
    "def pad_batch(batch):\n",
    "    max_len = len(batch[-1])\n",
    "    padded_batch = []\n",
    "    for x in batch:\n",
    "        x = [VOCAB_SIZE-1]*(max_len-len(x)) + x\n",
    "        padded_batch.append(x)\n",
    "    return padded_batch\n",
    "\n",
    "VOCAB_SIZE += 1\n",
    "batched_X_padded = list(map(pad_batch, batched_X))\n",
    "print_dataset(batched_X_padded, batched_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create DyNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = dn.Model()\n",
    "input_lookup = model.add_lookup_parameters((VOCAB_SIZE, EMBEDDINGS_SIZE))\n",
    "lstm = dn.LSTMBuilder(LSTM_NUM_OF_LAYERS, EMBEDDINGS_SIZE, STATE_SIZE, model)\n",
    "output_w = model.add_parameters((NUM_OF_CLASSES, STATE_SIZE))\n",
    "output_b = model.add_parameters((NUM_OF_CLASSES))\n",
    "\n",
    "def get_probs(batch):\n",
    "    dn.renew_cg()\n",
    "    \n",
    "    #the I iteration embed all the ith items in all batches\n",
    "    embeded = [dn.lookup_batch(input_lookup, chars) for chars in zip(*batch)]\n",
    "    state = lstm.initial_state()\n",
    "    output_vec = state.transduce(embeded)[-1]\n",
    "    w = dn.parameter(output_w)\n",
    "    b = dn.parameter(output_b)\n",
    "    return w*output_vec+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(trainX, trainY):\n",
    "    print('starting train')\n",
    "    trainer = dn.AdamTrainer(model)\n",
    "    for _ in range(REPEATS):\n",
    "        for X, Y in zip(trainX, trainY):\n",
    "            probs = get_probs(X)\n",
    "            loss = dn.sum_batches(dn.pickneglogsoftmax_batch(probs, Y))\n",
    "            loss_value = loss.value()\n",
    "            loss.backward()\n",
    "            trainer.update()\n",
    "    print('done training!')\n",
    "\n",
    "def validate(testX, testY):\n",
    "    print('starting validation')\n",
    "    acc = []\n",
    "    for X, Y in zip(testX, testY):\n",
    "        probs = get_probs(X).npvalue()\n",
    "        for i in range(len(probs[0])):\n",
    "            pred = argmax(probs[:, i])\n",
    "            label = Y[i]\n",
    "            if pred == label:\n",
    "                acc.append(1)\n",
    "            else:\n",
    "                acc.append(0)\n",
    "    print('accuracy: ', mean(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting validation\n",
      "accuracy:  0.6\n",
      "starting train\n",
      "done training!\n",
      "starting validation\n",
      "accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "validate(batched_X_padded, batched_Y)\n",
    "train(batched_X_padded, batched_Y)\n",
    "validate(batched_X_padded, batched_Y)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
