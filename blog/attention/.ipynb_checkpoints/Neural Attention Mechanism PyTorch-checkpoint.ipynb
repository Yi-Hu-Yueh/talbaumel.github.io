{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence To Sequence Attention Models In PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most coveted AI tasks is automatic machine translation (MT). In this task a sequence of words in a source language are translated into a sequence of words in a target language (usually those sequences are of different lengths). In order to succeed in this task, the model needs to generalize language patterns from a relatively small dataset of translated examples (the number of possible sentences in each language is large enough to be considered infinite). \n",
    "\n",
    "Sequence to sequence neural models have been shown to make such generalizations for input and output of variable lengths. \n",
    "For example: [Bahdanau et al.2015] Neural Machine Translation by Jointly Learning to Align and Translate in ICLR 2015\n",
    "(https://arxiv.org/abs/1409.0473) achieves nearly state of the art results in MT.\n",
    "\n",
    "In this tutorial, we solve a toy problem on a synthetic dataset to learn the reverse function (map \"abcd\" to \"dcba\") using the same models developed to solve MT.\n",
    "\n",
    "We review three models of increasing sophistication: \n",
    "* a simple Recurrent Neural Network (RNN)\n",
    "* a sequence to sequence encoder-decoder model (https://www.tensorflow.org/versions/r0.9/tutorials/seq2seq/index.html)\n",
    "* finally, an Attention Based model as introduced by Bahdanau et al.\n",
    "\n",
    "This is a hands-on description of these models, using the DyNet framework.\n",
    "Make sure you first install Dynet following these instructions: http://dynet.readthedocs.io/en/latest/python.html\n",
    "\n",
    "The task we address is the following: the input is a random string $(c_0, c_1, c_2 ... c_n)$ of random length n and the output is the reversed string $(c_n, c_{n-1}, c_{n-2} ... c_0)$.\n",
    "\n",
    "Let's build a function that generates an $(input, output)$ instance of our reverse function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from random import choice, randrange\n",
    "from tqdm import tqdm\n",
    "\n",
    "EOS = \"<EOS>\" #all strings will end with the End Of String token\n",
    "characters = list(\"abcd\")\n",
    "characters.append(EOS)\n",
    "\n",
    "int2char = list(characters)\n",
    "char2int = {c:i for i,c in enumerate(characters)}\n",
    "\n",
    "VOCAB_SIZE = len(characters)\n",
    "\n",
    "def sample_model(min_length, max_lenth):\n",
    "    random_length = randrange(min_length, max_lenth)                             # Pick a random length\n",
    "    random_char_list = [choice(characters[:-1]) for _ in range(random_length)]  # Pick random chars\n",
    "    random_string = ''.join(random_char_list) \n",
    "    return random_string, random_string[::-1]  # Return the random string and its reverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us generate a couple $(Input, Output)$ examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('badd', 'ddab')\n",
      "('dbcddcc', 'ccddcbd')\n"
     ]
    }
   ],
   "source": [
    "print(sample_model(4, 5))\n",
    "print(sample_model(5, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compare different models under identical conditions, we create a dataset in advance. \n",
    "The dataset contains 3,000* random strings with lengths ranging from 1 to 15.<br><br> \n",
    "\n",
    "*Note that there are more than $\\sum_{n=1}^{14} 4^{n} >> 4^{14} = 268,435,456$ possible random strings so a dataset containing only 3,000 strings means our model will really need to generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_STRING_LEN = 15\n",
    "\n",
    "train_set = [sample_model(1, MAX_STRING_LEN) for _ in range(3000)]\n",
    "val_set = [sample_model(1, MAX_STRING_LEN) for _ in range(50)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, We split the data set into train and validation subsets. \n",
    "\n",
    "We define a $train$ function to optimize a model on our training dataset and plot training errors as learning proceeds over the validation set.  \n",
    "\n",
    "This method uses generic interface of the PyCNN network class which is used to encode any neural network model:\n",
    "* network.get_loss(input, output)\n",
    "* dy.SimpleSGDTrainer(network.model)\n",
    "\n",
    "This applies a backpropagation training regime over the network for a set number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def train(network, train_set, val_set, epochs = 20):\n",
    "    def get_val_set_loss(network, val_set):\n",
    "        loss = [get_loss(network, input_string, output_string).data[0] for input_string, output_string in val_set]\n",
    "        return sum(loss)\n",
    "    \n",
    "    train_set = train_set*epochs\n",
    "    losses = []\n",
    "    iterations = []\n",
    "    optim = torch.optim.SGD(network.parameters(), lr = 0.01, momentum=0.9)\n",
    "    \n",
    "    for i, training_example in enumerate(tqdm(train_set)):\n",
    "        optim.zero_grad()\n",
    "        input_string, output_string = training_example\n",
    "        \n",
    "        loss = get_loss(network, input_string, output_string)\n",
    "        loss_value = loss.data[0]\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        # Accumulate average losses over training to plot\n",
    "        if i%(len(train_set)/100) == 0:\n",
    "            val_loss = get_val_set_loss(network, val_set)\n",
    "            losses.append(val_loss)\n",
    "            iterations.append(i/((len(train_set)/100)))\n",
    "\n",
    "    plt.plot(iterations, losses)\n",
    "    plt.axis([0, 100, 0, 100])\n",
    "    plt.show() \n",
    "    print('loss on validation set:', val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us test our toy problem using a simple Recurrent Neural Network (RNN). \n",
    "This model can handle inputs of variable lengths, but it cannot produce outputs of variable lengths. \n",
    "Luckily for us, in our problem the length of the input and the output are equal.\n",
    "\n",
    "Specifically, we will use LSTM units in our RNN - read background information about LSTMs: http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "Then make sure to read about RNNs in pycnn: https://github.com/clab/cnn/blob/master/pyexamples/tutorials/RNNs.ipynb\n",
    "\n",
    "If you want, compare it with the Tensorflow corresponding tutorial: https://www.tensorflow.org/versions/r0.9/tutorials/recurrent/index.html\n",
    "\n",
    "Note that in all of our models, we have character units - that is, we consider sequences of characters.\n",
    "In many other models, we could consider sequences of words instead of characters, and sometimes sequences of morphemes or word segments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/rnn.jpg\" width=\"50%\" height=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will develop three models that are all refinements of the same structure.\n",
    "We therefore define a base class that has the following interface:\n",
    "* A constructor with the hyper-parameters of the network (number of layers, sizes of embeddings and for the latent state of the units.\n",
    "* Pre-processors for input and output to normalize the sequences coming in and out of the network.  In our case, these simply add the special <EOS> tokens.\n",
    "* The method $get\\_probs(rnn\\_output)$ maps the computed output of the RNN to a probability distribution over candidate characters by applying a simple softmax transformation - and the corresponding $predict(probs)$ method which picks the maximum likelihood character given the prob distribution.\n",
    "* $get\\_loss(input\\_string, output\\_string)$ computes the overall loss of the network on a single pair.  It runs the network over the input, for each input char, it computes the distribution of possible output chars, and then computes the cross-entropy loss for each character: $-\\sum_{output\\_char} log(prob(output\\_char))$.\n",
    "* $generate(input\\_string)$ which generates a candidate output given the input based on the current state of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some helper functions\n",
    "def _preprocess_string(string):\n",
    "    string = list(string) + [EOS]\n",
    "    return Variable(torch.LongTensor([[char2int[c]] for c in string]))\n",
    "\n",
    "def get_loss(network, input_string, output_string):\n",
    "    input_string = _preprocess_string(input_string)\n",
    "    output_string = _preprocess_string(output_string)\n",
    "\n",
    "    probs = network(input_string)\n",
    "    size = probs.size()\n",
    "    size = size[0]*size[1]\n",
    "\n",
    "    return F.nll_loss(probs.view(size, -1), output_string.view(size, -1).squeeze()).sum()\n",
    "\n",
    "def generate(network, input_string):\n",
    "    input_string = _preprocess_string(input_string)\n",
    "    probs = network(input_string)\n",
    "    generated = [int2char[prob[0].topk(1)[1][0]] for prob in probs.data]\n",
    "    return (''.join(generated)).split('<EOS>')[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class SimpleRNNNetwork(nn.Module):\n",
    "    def __init__(self,num_of_layers, embeddings_size, state_size):\n",
    "        super(SimpleRNNNetwork, self).__init__()\n",
    "        batch_size = 1\n",
    "        # the embedding paramaters\n",
    "        self.embeddings = nn.Embedding(VOCAB_SIZE, embeddings_size)\n",
    "\n",
    "        # the rnn\n",
    "        self.RNN = nn.LSTM(embeddings_size, state_size, num_of_layers)\n",
    "        self.h0 = Variable(torch.zeros(num_of_layers, batch_size, state_size))\n",
    "        self.c0 = Variable(torch.zeros(num_of_layers, batch_size, state_size))\n",
    "        \n",
    "        # project the rnn output to a vector of VOCAB_SIZE length\n",
    "        self.linear= nn.Linear(state_size, VOCAB_SIZE)\n",
    "\n",
    "    def __call__(self, input_string):                        \n",
    "        embedded = self.embeddings(input_string)\n",
    "        output, hn = self.RNN(embedded, (self.h0, self.c0))\n",
    "        logits = self.linear(output)\n",
    "        return F.log_softmax(logits, 2)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to run the first RNN network over our sample dataset - and test it on a simple \"ab\" string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60000/60000 [07:43<00:00, 129.59it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a5ed898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on validation set: 54.10904014110565\n",
      "aaa\n"
     ]
    }
   ],
   "source": [
    "RNN_NUM_OF_LAYERS = 2\n",
    "EMBEDDINGS_SIZE = 4\n",
    "STATE_SIZE = 128\n",
    "\n",
    "rnn = SimpleRNNNetwork(RNN_NUM_OF_LAYERS, EMBEDDINGS_SIZE, STATE_SIZE)\n",
    "train(rnn, train_set, val_set)\n",
    "print(generate(rnn, 'abc'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the loss does not improve over time - training does not converge.\n",
    "We also see that the network did not learn how to reverse 'ab'.\n",
    "\n",
    "Our RNN model has no chance to learn the $reverse\\ function$ since it predicts each output character $o_i$ immediately after reading input character $i_i$. So when predicting the 1st output charcter the RNN didn't even see the last input charcter.\n",
    "\n",
    "No magic going on here - so let's improve.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Decoder Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now try to solve our problem using the encoder-decoder model. An encoder RNN will read the entire input string and generate a fixed length encoded vector to represent the entire input string. The decoder RNN will then use the encoded string to predict the output.\n",
    "\n",
    "This strategy should address the concern we identified above - which is that to generate a reverse string properly we should first look at the whole input string - then start generating. \n",
    "\n",
    "Naturally, we are still skeptical: can it be that the encoder will \"remember\" the input string sufficiently to generate the whole output strings?  Let us try it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/encdec.jpg\" width=\"50%\" height=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepare a new class EncoderDecoderNetwork which derives from SimpleRNNNetwork.\n",
    "\n",
    "In the constructor, we build two RNNs - encoder and decoder.\n",
    "We define a new method $encode\\_string(embedded\\_string)$ which runs the input string into the encoder.\n",
    "We then adapt $generate(input\\_string)$ and $get\\_loss(input\\_string, output\\_string)$ to perform the \n",
    "encoder / decoder choreography.\n",
    "\n",
    "Note that the decoder receives the same encoding of the whole input string for all slices.\n",
    "Each slice of the decoder must learn to decide what \"part\" of the encoding it should look at to decide\n",
    "which output to produce - based on the latent state the previous slice produced (remember that all slices are\n",
    "repetitions of the same network unit with the same parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoderNetwork(nn.Module):\n",
    "    def __init__(self, num_of_layers, embeddings_size, state_size):\n",
    "        super(EncoderDecoderNetwork, self).__init__()\n",
    "        batch_size = 1\n",
    "        # the embedding paramaters\n",
    "        self.embeddings = nn.Embedding(VOCAB_SIZE, embeddings_size)\n",
    "\n",
    "        # the encoder\n",
    "        self.enc = nn.LSTM(embeddings_size, state_size, num_of_layers)\n",
    "        self.enc_h0 = Variable(torch.zeros(num_of_layers, batch_size, state_size))\n",
    "        self.enc_c0 = Variable(torch.zeros(num_of_layers, batch_size, state_size))\n",
    "\n",
    "        #the decoder\n",
    "        self.dec = nn.LSTM(state_size, state_size, num_of_layers)\n",
    "        self.dec_h0 = Variable(torch.zeros(num_of_layers, batch_size, state_size))\n",
    "        self.dec_c0 = Variable(torch.zeros(num_of_layers, batch_size, state_size))\n",
    "\n",
    "        # project the rnn output to a vector of VOCAB_SIZE length\n",
    "        self.linear = nn.Linear(state_size, VOCAB_SIZE)\n",
    "\n",
    "    def __call__(self, input_string):\n",
    "        embedded = self.embeddings(input_string)\n",
    "        output, hn = self.enc(embedded, (self.enc_h0, self.enc_c0))\n",
    "        encoded = output[-1]\n",
    "        dec_h = self.dec_h0\n",
    "        dec_c = self.dec_c0\n",
    "        outputs = []\n",
    "        for _ in range(len(input_string)):\n",
    "            output, (dec_h, dec_c) = self.dec(encoded, (dec_h, dec_c))\n",
    "            outputs.append(output)\n",
    "        logits = self.linear(torch.torch.cat(outputs, 0))\n",
    "\n",
    "        return F.log_softmax(logits, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 11084/60000 [01:18<05:45, 141.72it/s]"
     ]
    }
   ],
   "source": [
    "ENC_RNN_NUM_OF_LAYERS = 1\n",
    "#DEC_RNN_NUM_OF_LAYERS = 1\n",
    "EMBEDDINGS_SIZE = 4\n",
    "ENC_STATE_SIZE = 64\n",
    "#DEC_STATE_SIZE = 64\n",
    "\n",
    "encoder_decoder = EncoderDecoderNetwork(ENC_RNN_NUM_OF_LAYERS, EMBEDDINGS_SIZE, ENC_STATE_SIZE)\n",
    "train(encoder_decoder, train_set, val_set)\n",
    "print(generate(encoder_decoder, 'abcd'))\n",
    "print(generate(encoder_decoder, 'abcdabcdabcdabcd'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss curve indicates this model is doing much better than the hepless SimpleRNN.  It is \"learning\" something.\n",
    "But it hits a plateau and remains unstable after the 10th iteration.\n",
    "\n",
    "Since the encoder decoder uses a fixed length vector to represent variable size input strings it can't properly represent long input strings. \n",
    "\n",
    "Lets test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_strings = [sample_model(1, 5) for _ in range(100)]\n",
    "medium_strings = [sample_model(5, 10) for _ in range(100)]\n",
    "long_strings = [sample_model(10, 15) for _ in range(100)]\n",
    "\n",
    "def count_matches(network, val_set):\n",
    "    matches = [generate(network, input_string)==output_string for input_string, output_string in val_set]\n",
    "    return matches.count(True)\n",
    "\n",
    "print('Matches for short strings', count_matches(encoder_decoder, short_strings))\n",
    "print('Matches for medium strings', count_matches(encoder_decoder, medium_strings))\n",
    "print('Matches for long strings', count_matches(encoder_decoder, long_strings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the encoder-decoder model does improve over the Simple RNN but hits a plateau because it has limited memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *attention model* was introduced to address the limitation we just observed:\n",
    "* How does the decoder know which part of the encoding is relevant at each step of the generation.\n",
    "* How can we overcome the limited memory of the encoder so that we can \"remember\" more of the encoding process than a single fixed size vector.\n",
    "\n",
    "The attention model comes between the encoder and the decoder and helps the decoder to pick only the encoded inputs that are important for each step of the decoding process.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/att.jpg\" width=\"50%\" height=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each encoded input from the encoder RNN, the attention mechanism calculates its importance:\n",
    "\n",
    "$importance_{ij} =V*tanh(encodedInput_iW_1+decoderstate_jW_2)$<br>\n",
    "$importance_{ij}$ is the importance of encoded vector $i$ at decoding step $j$<br>\n",
    "$W_1$, $W_2$ and $V$ are learned parameters <br>\n",
    "\n",
    "Once we calculate the importance of each encoded vector, we normalize the vectors with softmax and multiply each encoded vector by its weight to obtain a \"time dependent\" input encoding which is fed to each step of the decoder RNN.\n",
    "\n",
    "Note that in this model, the attention mechanism computes a fixed-size vector that encodes the whole input sequence based on the sequence of *all* the outputs generated by the encoder (as opposed to the encoder-decoder model above which was looking ONLY at the last state generated by the encoder for all the slices of the decoder).\n",
    "\n",
    "We prepare a new class that refines the EncoderDecoderNetwork.  AttentionNetwork adds the units of the attention model ($W_1, W_2 and V$) in its constructor. \n",
    "\n",
    "The method $attend(input_vectors, state)$ computes the weighted representation of the whole input string for each slide of the decoder.\n",
    "\n",
    "$get\\_loss(input\\_string, output\\_string)$ and $generate(input\\_string)$ are adapted to introduce the call to $attend()$ in the overall choreography."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoderAttentionNetwork(nn.Module):\n",
    "    def __init__(self, num_of_layers, embeddings_size, state_size):\n",
    "        super(EncoderDecoderAttentionNetwork, self).__init__()\n",
    "        batch_size = 1\n",
    "        # the embedding paramaters\n",
    "        self.embeddings = nn.Embedding(VOCAB_SIZE, embeddings_size)\n",
    "\n",
    "        # the encoder\n",
    "        self.enc = nn.LSTM(embeddings_size, state_size, num_of_layers)\n",
    "        self.enc_h0 = Variable(torch.randn(num_of_layers, batch_size, state_size))\n",
    "        self.enc_c0 = Variable(torch.randn(num_of_layers, batch_size, state_size))\n",
    "\n",
    "        # the decoder\n",
    "        self.dec = nn.LSTM(state_size, state_size, num_of_layers)\n",
    "        self.dec_h0 = Variable(torch.randn(num_of_layers, batch_size, state_size))\n",
    "        self.dec_c0 = Variable(torch.randn(num_of_layers, batch_size, state_size))\n",
    "\n",
    "        # the attention\n",
    "        self.att_w1 = nn.Linear(state_size, state_size)\n",
    "        self.att_w2 = nn.Linear(state_size, state_size)\n",
    "        self.att_v = nn.Linear(state_size, 1)\n",
    "\n",
    "        # project the rnn output to a vector of VOCAB_SIZE length\n",
    "        self.linear = nn.Linear(state_size, VOCAB_SIZE)\n",
    "\n",
    "    def attention(self, encoder_outputs, decoder_state):\n",
    "        unnormalized_att = self.att_v(F.tanh(self.att_w1(decoder_state) + self.att_w2(encoder_outputs)))\n",
    "        att = F.softmax(unnormalized_att) #dosn't work for batchs\n",
    "        attended = encoder_outputs.mul(att).sum(0)\n",
    "        return attended\n",
    "\n",
    "    def __call__(self, input_string):\n",
    "        embedded = self.embeddings(input_string)\n",
    "        encoder_outputs, hn = self.enc(embedded, (self.enc_h0, self.enc_c0))\n",
    "\n",
    "        dec_h = self.dec_h0\n",
    "        dec_c = self.dec_c0\n",
    "        outputs = []\n",
    "        for _ in range(len(input_string)):\n",
    "            encoded = self.attention(encoder_outputs, dec_c)\n",
    "            output, (dec_h, dec_c) = self.dec(encoded, (dec_h, dec_c))\n",
    "            outputs.append(output)\n",
    "        logits = self.linear(torch.torch.cat(outputs, 0))\n",
    "\n",
    "        return F.log_softmax(logits, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ready to try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENC_RNN_NUM_OF_LAYERS = 1\n",
    "#DEC_RNN_NUM_OF_LAYERS = 1\n",
    "EMBEDDINGS_SIZE = 4\n",
    "ENC_STATE_SIZE = 32\n",
    "#DEC_STATE_SIZE = 32\n",
    "\n",
    "att = EncoderDecoderAttentionNetwork(\n",
    "    ENC_RNN_NUM_OF_LAYERS, EMBEDDINGS_SIZE, ENC_STATE_SIZE)\n",
    "train(att, train_set, val_set)\n",
    "print(generate(att, 'abcdabcdabcdabcd'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the attention mechanism solved our problem! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Matches for short strings', count_matches(att, short_strings))\n",
    "print('Matches for medium strings', count_matches(att, medium_strings))\n",
    "print('Matches for long strings', count_matches(att, long_strings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss went down on the same dataset from 200 to 0.  We now can predict correctly long strings.\n",
    "\n",
    "How did this happen?  Let us zoom in on the attention part of the model and try to illustrate what was learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a version of the attention network that plots the attention of each encoded input at each decoding step when generating.  We want to look at what was the relative weight that was computed for each of the outputs of the encoders \n",
    "$encOutput_i$ when generating each of the output characters $Output_j$\n",
    "\n",
    "This has the shape of a matrix NxN which will tell us where the model is \"focusing\" on the input encodings when it decides to generate each output char."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "    \n",
    "class AttentionNetworkWithPrint(EncoderDecoderAttentionNetwork):\n",
    "    def __init__(self, layers, embeddings_size, state_size):\n",
    "        EncoderDecoderAttentionNetwork.__init__(self, layers, embeddings_size, state_size)\n",
    "\n",
    "        self.should_print = False\n",
    "        self.att_mat = []\n",
    "    \n",
    "    def attention(self, encoder_outputs, decoder_state):\n",
    "        unnormalized_att = self.att_v(F.tanh(self.att_w1(decoder_state) + self.att_w2(encoder_outputs)))\n",
    "        att = F.softmax(unnormalized_att) #dosn't work for batchs\n",
    "        attended = encoder_outputs.mul(att).sum(0)\n",
    "        if self.should_print:\n",
    "            self.att_mat.append(attended.data.numpy())\n",
    "        return attended\n",
    "    \n",
    "    def _plot_attention(self, matrix, max_weight=None, ax=None):\n",
    "        \"\"\"Draw Hinton diagram for visualizing a weight matrix.\"\"\"\n",
    "        ax = ax if ax is not None else plt.gca()\n",
    "\n",
    "        if not max_weight:\n",
    "            max_weight = 2**np.ceil(np.log(np.abs(matrix).max())/np.log(2))\n",
    "\n",
    "        ax.patch.set_facecolor('gray')\n",
    "        ax.set_aspect('equal', 'box')\n",
    "        ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "        ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "        for (x, y), w in np.ndenumerate(matrix):\n",
    "            color = 'white' if w > 0 else 'black'\n",
    "            size = np.sqrt(np.abs(w))\n",
    "            rect = plt.Rectangle([x - size / 2, y - size / 2], size, size,\n",
    "                                 facecolor=color, edgecolor=color)\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "        ax.autoscale_view()\n",
    "        ax.invert_yaxis()\n",
    "        plt.show()\n",
    "    \n",
    "    def generate_and_plot_attention(self, input_string):\n",
    "        att.should_print = True\n",
    "        att.att_mat = []\n",
    "        output_string = generate(self, input_string)\n",
    "        self._plot_attention(np.array(att.att_mat))\n",
    "        att.should_print = False\n",
    "        att.att_mat = []\n",
    "        return output_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ENC_RNN_NUM_OF_LAYERS = 1\n",
    "DEC_RNN_NUM_OF_LAYERS = 1\n",
    "EMBEDDINGS_SIZE = 4\n",
    "ENC_STATE_SIZE = 32\n",
    "DEC_STATE_SIZE = 32\n",
    "\n",
    "att = AttentionNetworkWithPrint(ENC_RNN_NUM_OF_LAYERS, DEC_RNN_NUM_OF_LAYERS, EMBEDDINGS_SIZE, ENC_STATE_SIZE, DEC_STATE_SIZE)\n",
    "train(att, train_set, val_set)\n",
    "\n",
    "att.att_mat = []\n",
    "print(att.generate_and_plot_attention('abcdabcdabcdabcd'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When plotting the attention weights of each input stage we can see that for we get a diagonal line. When generation stage $i$ the most important input was $|input|-i$\n",
    "\n",
    "In other words, the attention model learned pretty well the structure of the \"reverse\" function on our test sample by looking at 3,000 sample strings.\n",
    "\n",
    "We recommend to read the following book for an extended introduction to NLP using neural networks: http://www.kyunghyuncho.me/home/blog/lecturenotefornlpwithdistributedreponarxivnow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About me: https://www.cs.bgu.ac.il/~talbau/"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
