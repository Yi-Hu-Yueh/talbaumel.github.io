{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DyNet vs PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from random import choice, randrange\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "EOS = \"<EOS>\" #all strings will end with the End Of String token\n",
    "PAD = \"<PAD>\"\n",
    "characters = list(\"abcd\")\n",
    "characters.append(EOS)\n",
    "characters.append(PAD)\n",
    "\n",
    "int2char = list(characters)\n",
    "char2int = {c:i for i,c in enumerate(characters)}\n",
    "\n",
    "VOCAB_SIZE = len(characters)\n",
    "PAD_IDX = VOCAB_SIZE - 1\n",
    "\n",
    "def sample_model(min_length, max_lenth):\n",
    "    random_length = randrange(min_length, max_lenth)                             # Pick a random length\n",
    "    random_char_list = [choice(characters[:-2]) for _ in range(random_length)]  # Pick random chars\n",
    "    random_string = ''.join(random_char_list) \n",
    "    return random_string, random_string[::-1]  # Return the random string and its reverse\n",
    "    \n",
    "\n",
    "MIN_STRING_LEN = 1\n",
    "MAX_STRING_LEN = 10\n",
    "TRAIN_SET_SIZE = 5000\n",
    "VAL_SET_SIZE = 10\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "RNN_NUM_OF_LAYERS = 1\n",
    "EMBEDDINGS_SIZE = 4\n",
    "STATE_SIZE = 64\n",
    "\n",
    "default_epochs = 20\n",
    "\n",
    "train_set = [sample_model(MIN_STRING_LEN, MAX_STRING_LEN) for _ in range(TRAIN_SET_SIZE)]\n",
    "val_set = [sample_model(MIN_STRING_LEN, MAX_STRING_LEN) for _ in range(VAL_SET_SIZE)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DyNet Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import dynet as dy\n",
    "from tqdm import tqdm\n",
    "def train_dynet(network, train_set, val_set, epochs = default_epochs):\n",
    "    def get_val_set_loss(network, val_set):\n",
    "        loss = [network.get_loss(input_string, output_string).value() for input_string, output_string in val_set]\n",
    "        return sum(loss)\n",
    "    \n",
    "    train_set = train_set*epochs\n",
    "    trainer = dy.SimpleSGDTrainer(network.model)\n",
    "    losses = []\n",
    "    iterations = []\n",
    "    for i, training_example in enumerate(tqdm(train_set)):\n",
    "        input_string, output_string = training_example\n",
    "        \n",
    "        loss = network.get_loss(input_string, output_string)\n",
    "        loss_value = loss.value()\n",
    "        loss.backward()\n",
    "        trainer.update()\n",
    "\n",
    "        # Accumulate average losses over training to plot\n",
    "        if i%(len(train_set)/100) == 0:\n",
    "            val_loss = get_val_set_loss(network, val_set)\n",
    "            losses.append(val_loss)\n",
    "            iterations.append(i/((len(train_set)/100)))\n",
    "\n",
    "    plt.plot(iterations, losses)\n",
    "    plt.axis([0, 100, 0, len(val_set)*MAX_STRING_LEN])\n",
    "    plt.show() \n",
    "    print('loss on validation set:', val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "RNN_BUILDER = dy.LSTMBuilder\n",
    "    \n",
    "class EncoderDecoderDyNet():\n",
    "    def __init__(self, num_of_layers, embeddings_size, state_size):\n",
    "        self.model = dy.Model()\n",
    "\n",
    "        # the embedding paramaters\n",
    "        self.embeddings = self.model.add_lookup_parameters((VOCAB_SIZE, embeddings_size))\n",
    "\n",
    "        # the rnns\n",
    "        self.ENC_RNN = RNN_BUILDER(num_of_layers, embeddings_size, state_size, self.model)\n",
    "        self.DEC_RNN = RNN_BUILDER(num_of_layers, state_size, state_size, self.model)\n",
    "\n",
    "        # project the rnn output to a vector of VOCAB_SIZE length\n",
    "        self.output_w = self.model.add_parameters((VOCAB_SIZE, state_size))\n",
    "        self.output_b = self.model.add_parameters((VOCAB_SIZE))\n",
    "        \n",
    "    def _add_eos(self, string):\n",
    "        string = list(string) + [EOS]\n",
    "        return [char2int[c] for c in string]\n",
    "    \n",
    "    def _embed_string(self, string):\n",
    "        return [self.embeddings[char] for char in string]\n",
    "\n",
    "    def _run_rnn(self, init_state, input_vecs):\n",
    "        s = init_state\n",
    "\n",
    "        states = s.add_inputs(input_vecs)\n",
    "        rnn_outputs = [s.output() for s in states]\n",
    "        return rnn_outputs\n",
    "    \n",
    "    def _get_probs(self, rnn_output):\n",
    "        output_w = dy.parameter(self.output_w)\n",
    "        output_b = dy.parameter(self.output_b)\n",
    "\n",
    "        probs = dy.softmax(output_w * rnn_output + output_b)\n",
    "        return probs\n",
    "    \n",
    "            \n",
    "    def get_loss(self, input_string, output_string):\n",
    "        input_string = self._add_eos(input_string)\n",
    "        output_string = self._add_eos(output_string)\n",
    "\n",
    "        dy.renew_cg()\n",
    "\n",
    "        probs = self(input_string)\n",
    "        loss = [-dy.log(dy.pick(p, output_char)) for p, output_char in zip(probs, output_string)]\n",
    "        loss = dy.esum(loss)\n",
    "        return loss\n",
    "\n",
    "    def _predict(self, probs):\n",
    "        probs = probs.value()\n",
    "        predicted_char = int2char[probs.index(max(probs))]\n",
    "        return predicted_char\n",
    "    \n",
    "    def generate(self, input_string):\n",
    "        input_string = self._add_eos(input_string)\n",
    "\n",
    "        dy.renew_cg()\n",
    "        \n",
    "        probs = self(input_string)\n",
    "        output_string = [self._predict(p) for p in probs]\n",
    "        output_string = ''.join(output_string)\n",
    "        return output_string.replace('<EOS>', '')\n",
    "    def _encode_string(self, embedded_string):\n",
    "        initial_state = self.ENC_RNN.initial_state()\n",
    "\n",
    "        # run_rnn returns all the hidden state of all the slices of the RNN\n",
    "        hidden_states = self._run_rnn(initial_state, embedded_string)\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "    def __call__(self, input_string):\n",
    "        embedded_string = self._embed_string(input_string)\n",
    "        # The encoded string is the hidden state of the last slice of the encoder\n",
    "        encoded_string = self._encode_string(embedded_string)[-1]\n",
    "\n",
    "        rnn_state = self.DEC_RNN.initial_state()\n",
    "\n",
    "        probs = []\n",
    "        for _ in range(len(input_string)):\n",
    "            rnn_state = rnn_state.add_input(encoded_string)\n",
    "            p = self._get_probs(rnn_state.output())\n",
    "            probs.append(p)\n",
    "        return probs\n",
    "    \n",
    "    \n",
    "class AttentionDyNet(EncoderDecoderDyNet):\n",
    "    def __init__(self, num_of_layers, embeddings_size, state_size):\n",
    "        EncoderDecoderDyNet.__init__(self, num_of_layers, embeddings_size, state_size)\n",
    "\n",
    "        # attention weights\n",
    "        self.attention_w1 = self.model.add_parameters((state_size, state_size))\n",
    "        self.attention_w2 = self.model.add_parameters((state_size, state_size))\n",
    "        self.attention_v = self.model.add_parameters((1, state_size))\n",
    "\n",
    "        self.state_size = state_size\n",
    "\n",
    "    def _attend(self, input_vectors, state):\n",
    "        w1 = dy.parameter(self.attention_w1)\n",
    "        w2 = dy.parameter(self.attention_w2)\n",
    "        v = dy.parameter(self.attention_v)\n",
    "        attention_weights = []\n",
    "\n",
    "        w2dt = w2 * state.h()[-1]\n",
    "        for input_vector in input_vectors:\n",
    "            attention_weight = v * dy.tanh(w1 * input_vector + w2dt)\n",
    "            attention_weights.append(attention_weight)\n",
    "        attention_weights = dy.softmax(dy.concatenate(attention_weights))\n",
    "\n",
    "        output_vectors = dy.esum(\n",
    "            [vector * attention_weight for vector, attention_weight in zip(input_vectors, attention_weights)])\n",
    "        return output_vectors\n",
    "\n",
    "    def __call__(self, input_string):\n",
    "        dy.renew_cg()\n",
    "\n",
    "        embedded_string = self._embed_string(input_string)\n",
    "        encoded_string = self._encode_string(embedded_string)\n",
    "\n",
    "        rnn_state = self.DEC_RNN.initial_state().add_input(dy.vecInput(self.state_size))\n",
    "\n",
    "        probs = []\n",
    "        for _ in range(len(input_string)):\n",
    "            attended_encoding = self._attend(encoded_string, rnn_state)\n",
    "            rnn_state = rnn_state.add_input(attended_encoding)\n",
    "            p = self._get_probs(rnn_state.output())\n",
    "            probs.append(p)\n",
    "        return probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess_string(strings):\n",
    "    batch_size = len(strings)\n",
    "\n",
    "    strings_with_eos = [list(string) + [EOS] for string in strings]\n",
    "    max_len = max([len(string) for string in strings_with_eos])\n",
    "    padded_strings = [string + [PAD] * (max_len - len(string)) for string in strings_with_eos]\n",
    "    #len_first = [[padded_strings[j][i]for j in range(batch_size)] for i in range(max_len)]\n",
    "    #int_strings = [[char2int[c] for c in string] for string in len_first]\n",
    "    int_strings = [[char2int[c] for c in string] for string in padded_strings]\n",
    "    var = Variable(torch.LongTensor(int_strings))\n",
    "    return var\n",
    "\n",
    "def get_loss(network, input_strings, output_strings):\n",
    "    batch_size = len(output_strings)\n",
    "    input_strings_t = _preprocess_string(input_strings)\n",
    "    output_strings_t = _preprocess_string(output_strings)\n",
    "\n",
    "    probs = network(input_strings_t, batch_size).permute(1, 0, 2)\n",
    "\n",
    "    loss = sum([F.cross_entropy(p, t, ignore_index=PAD_IDX) for p, t in zip(probs, output_strings_t)])\n",
    "\n",
    "    return loss\n",
    "\n",
    "def generate(network, input_string):\n",
    "    input_string = _preprocess_string([input_string])\n",
    "    probs = network(input_string, 1)\n",
    "    generated = [int2char[prob[0].topk(1)[1][0]] for prob in probs.data]\n",
    "    return (''.join(generated)).split(EOS)[0].replace(PAD, '')\n",
    "\n",
    "def batcher(dataset, epochs = 1):\n",
    "    sources, targets = [], []\n",
    "    i = 0\n",
    "    for _ in range(epochs):\n",
    "        for source, target in dataset:\n",
    "            i+=1\n",
    "            sources.append(source), targets.append(target)\n",
    "            if len(sources) >= batch_size:\n",
    "                yield sources, targets\n",
    "                sources, targets = [], []\n",
    "    if sources: yield sources, targets\n",
    "        \n",
    "def train(network, train_set, val_set, epochs=default_epochs):\n",
    "    def get_val_set_loss(network, val_set):\n",
    "        losses = [get_loss(network, input_strings, output_strings).data[0]\n",
    "                 for input_strings, output_strings in batcher(val_set)]\n",
    "        return sum(losses)\n",
    "    losses = []\n",
    "    iterations = []\n",
    "    optim = torch.optim.Adam(network.parameters())\n",
    "    \n",
    "    total_iterations = max(int((len(train_set)*epochs)/batch_size), 100)\n",
    "    for i, (input_strings, output_strings) in enumerate(tqdm(batcher(train_set, epochs=epochs))):\n",
    "              \n",
    "        optim.zero_grad()\n",
    "        loss = get_loss(network, input_strings, output_strings)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "            \n",
    "        # Accumulate average losses over training to plot\n",
    "        if i%int(total_iterations/100) == 0:\n",
    "            val_loss = get_val_set_loss(network, val_set)\n",
    "            losses.append(val_loss)\n",
    "            iterations.append(i/((len(train_set)/100)))\n",
    "\n",
    "    plt.plot(iterations, losses)\n",
    "    #plt.axis([0, 100, 0, 100])\n",
    "    plt.show() \n",
    "    print('loss on validation set:', val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoderPyTorch(nn.Module):\n",
    "    def __init__(self, num_of_layers, embeddings_size, state_size):\n",
    "        super(EncoderDecoderPyTorch, self).__init__()\n",
    "        self.num_of_layers = num_of_layers\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        self.embeddings = nn.Embedding(VOCAB_SIZE, embeddings_size, padding_idx=PAD_IDX)\n",
    "\n",
    "        self.enc = nn.LSTM(embeddings_size, state_size, num_of_layers)\n",
    "        self.dec = nn.LSTM(state_size, state_size, num_of_layers)\n",
    "        \n",
    "        self.linear = nn.Linear(state_size, VOCAB_SIZE)\n",
    "    \n",
    "    def get_rnn_init_state(self, batch_size):\n",
    "        h0 = Variable(torch.zeros(self.num_of_layers, batch_size, self.state_size))\n",
    "        c0 = Variable(torch.zeros(self.num_of_layers, batch_size, self.state_size))\n",
    "        return h0, c0\n",
    "    \n",
    "    def encode(self, encoder_outputs, decoder_state):\n",
    "        return encoder_outputs[-1].unsqueeze(0)\n",
    "    \n",
    "    def forward(self, input_string, batch_size):\n",
    "        embedded = self.embeddings(input_string.permute(1, 0))\n",
    "        encoder_outputs, hn = self.enc(embedded, self.get_rnn_init_state(batch_size))\n",
    "        \n",
    "        hidden = self.get_rnn_init_state(batch_size)\n",
    "        \n",
    "        outputs = []\n",
    "        for _ in range(len(embedded)):\n",
    "            encoded = self.encode(encoder_outputs, hidden[0])\n",
    "            output, hidden = self.dec(encoded, hidden)\n",
    "            outputs.append(output)\n",
    "        logits = self.linear(torch.cat(outputs, 0))\n",
    "\n",
    "        return logits #F.log_softmax(logits, 2)\n",
    "\n",
    "class EncoderDecoderAttentionPyTorch(EncoderDecoderPyTorch):\n",
    "    def __init__(self, num_of_layers, embeddings_size, state_size):\n",
    "        super(EncoderDecoderPyTorch, self).__init__(num_of_layers, embeddings_size, state_size)\n",
    "        # the attention\n",
    "        self.att_w1 = nn.Linear(state_size, state_size, bias=False)\n",
    "        self.att_w2 = nn.Linear(state_size, state_size)\n",
    "        self.att_v = nn.Linear(state_size, 1)\n",
    "\n",
    "    def encode(self, encoder_outputs, decoder_state):\n",
    "        decoder_state = decoder_state[-1].unsqueeze(0) # Use only the last layer\n",
    "        unnormalized_att = self.att_v(F.tanh(self.att_w1(decoder_state) + self.att_w2(encoder_outputs)))\n",
    "        #att = F.softmax(unnormalized_att)\n",
    "        att = F.softmax(unnormalized_att.permute(1,0,2), dim=1).permute(1,0,2)\n",
    "        attended = encoder_outputs.mul(att).sum(0)\n",
    "        return attended.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_LAYERS = 1\n",
    "EMBEDDINGS_SIZE = 4\n",
    "STATE_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [01:23<00:00, 1196.25it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a0ae240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on validation set: 0.0007082305328367511\n",
      "dcacbadcbaaaba\n"
     ]
    }
   ],
   "source": [
    "att = AttentionDyNet(NUM_OF_LAYERS, EMBEDDINGS_SIZE, STATE_SIZE)\n",
    "train_dynet(att, train_set, val_set)\n",
    "print(att.generate('abcdabcdabcdabcd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "91406it [07:28, 204.00it/s]"
     ]
    }
   ],
   "source": [
    "att = EncoderDecoderPyTorch(RNN_NUM_OF_LAYERS, EMBEDDINGS_SIZE, STATE_SIZE)\n",
    "train(att, train_set, val_set)\n",
    "print(generate(att, 'abcdabcdabcdabcd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
